{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rossmann Store Sales\n",
    "### Forecast sales using store, promotion, and competitor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np                     # For mathematical calculations\n",
    "import matplotlib.pyplot as plt        # For plotting graphs\n",
    "%matplotlib inline\n",
    "import os\n",
    "import warnings                        # To ignore any warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'decision_tree_basic_sub.csv',\n",
       " 'eda_1.png',\n",
       " 'eda_store_1.png',\n",
       " 'eda_store_2.png',\n",
       " 'eda_train_3.png',\n",
       " 'Nirmal_Raj_Eganathan_Batch_56.ipynb',\n",
       " 'Rossman (1).ipynb',\n",
       " 'Rossman.ipynb',\n",
       " 'sample_submission.csv',\n",
       " 'store.csv',\n",
       " 'test.csv',\n",
       " 'test_df.csv',\n",
       " 'train.csv',\n",
       " 'train_df.csv',\n",
       " 'Untitled.ipynb',\n",
       " 'Untitled1.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()                          # List of files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Data\n",
    "data = pd.read_csv('train.csv')\n",
    "data1 = pd.read_csv('test.csv')\n",
    "data2 = pd.read_csv('store.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a Copy of Original Dataset\n"
     ]
    }
   ],
   "source": [
    "# Letâ€™s make a copy of train and test data so that even if we have to make any changes in these datasets \n",
    "# we would not lose the original datasets.\n",
    "train = data.copy()\n",
    "test = data1.copy()\n",
    "store = data2.copy()\n",
    "print(\"Using a Copy of Original Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train[:100]\n",
    "# test = test[:100]\n",
    "# store = store[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns\n",
      "Index(['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo',\n",
      "       'StateHoliday', 'SchoolHoliday'],\n",
      "      dtype='object') \n",
      "\n",
      "Train has 9 Variables\n",
      "---------------------------------------------------------------\n",
      "Index(['Id', 'Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday',\n",
      "       'SchoolHoliday'],\n",
      "      dtype='object') \n",
      "\n",
      "Test has 8 Variables\n",
      "---------------------------------------------------------------\n",
      "Index(['Store', 'StoreType', 'Assortment', 'CompetitionDistance',\n",
      "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
      "       'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval'],\n",
      "      dtype='object') \n",
      "\n",
      "Store has 10 Variables\n"
     ]
    }
   ],
   "source": [
    "#Understand the data - Firstly, we will check the features present in our data and then we will look at their data types.\n",
    "print(\"Dataset Columns\")\n",
    "print(train.columns,'\\n')\n",
    "print(\"Train has %s Variables\"%(len(train.columns)))\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(test.columns,'\\n')\n",
    "print(\"Test has %s Variables\"%(len(test.columns)))\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(store.columns,'\\n')\n",
    "print(\"Store has %s Variables\"%(len(store.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Records \n",
      "    Store  DayOfWeek        Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
      "0      1          5  2015-07-31   5263        555     1      1            0   \n",
      "1      2          5  2015-07-31   6064        625     1      1            0   \n",
      "2      3          5  2015-07-31   8314        821     1      1            0   \n",
      "3      4          5  2015-07-31  13995       1498     1      1            0   \n",
      "4      5          5  2015-07-31   4822        559     1      1            0   \n",
      "\n",
      "   SchoolHoliday  \n",
      "0              1  \n",
      "1              1  \n",
      "2              1  \n",
      "3              1  \n",
      "4              1   \n",
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "Test Records \n",
      "    Id  Store  DayOfWeek        Date  Open  Promo StateHoliday  SchoolHoliday\n",
      "0   1      1          4  2015-09-17   1.0      1            0              0\n",
      "1   2      3          4  2015-09-17   1.0      1            0              0\n",
      "2   3      7          4  2015-09-17   1.0      1            0              0\n",
      "3   4      8          4  2015-09-17   1.0      1            0              0\n",
      "4   5      9          4  2015-09-17   1.0      1            0              0 \n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "Store Records \n",
      "    Store StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
      "0      1         c          a               1270.0                        9.0   \n",
      "1      2         a          a                570.0                       11.0   \n",
      "2      3         a          a              14130.0                       12.0   \n",
      "3      4         c          c                620.0                        9.0   \n",
      "4      5         a          a              29910.0                        4.0   \n",
      "\n",
      "   CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
      "0                    2008.0       0              NaN              NaN   \n",
      "1                    2007.0       1             13.0           2010.0   \n",
      "2                    2006.0       1             14.0           2011.0   \n",
      "3                    2009.0       0              NaN              NaN   \n",
      "4                    2015.0       0              NaN              NaN   \n",
      "\n",
      "     PromoInterval  \n",
      "0              NaN  \n",
      "1  Jan,Apr,Jul,Oct  \n",
      "2  Jan,Apr,Jul,Oct  \n",
      "3              NaN  \n",
      "4              NaN   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets see the top 5 Records of both the data-set\n",
    "print(\"Train Records \\n\",train.head(5),'\\n')\n",
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(\"Test Records \\n\",test.head(5),'\\n')\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(\"Store Records \\n\",store.head(5),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About Data-types\n",
      "\n",
      "------------------------------------------------------\n",
      "object: Object format means variables are categorical.\n",
      "int64: It represents the integer variables.\n",
      "float64: It represents the variable which have some decimal values involved. They are also numerical variables.\n",
      "\n",
      " \n",
      "Train Variable Data-types\n",
      "\n",
      "Store             int64\n",
      "DayOfWeek         int64\n",
      "Date             object\n",
      "Sales             int64\n",
      "Customers         int64\n",
      "Open              int64\n",
      "Promo             int64\n",
      "StateHoliday     object\n",
      "SchoolHoliday     int64\n",
      "dtype: object \n",
      "\n",
      "Change the Date variable (object) to (datetime64) format because its in datetime format and we can also extract features like day,month,year etc.,\n",
      "------------------------------------------------\n",
      "Test Variable Data-types\n",
      "\n",
      "Id                 int64\n",
      "Store              int64\n",
      "DayOfWeek          int64\n",
      "Date              object\n",
      "Open             float64\n",
      "Promo              int64\n",
      "StateHoliday      object\n",
      "SchoolHoliday      int64\n",
      "dtype: object \n",
      "\n",
      "Similarly the data type of Date column to datetime format and Open variable has change to int and StateHoliday values are string type try to keep as int \n",
      "------------------------------------------------\n",
      "Store Variable Data-types\n",
      "\n",
      "Store                          int64\n",
      "StoreType                     object\n",
      "Assortment                    object\n",
      "CompetitionDistance          float64\n",
      "CompetitionOpenSinceMonth    float64\n",
      "CompetitionOpenSinceYear     float64\n",
      "Promo2                         int64\n",
      "Promo2SinceWeek              float64\n",
      "Promo2SinceYear              float64\n",
      "PromoInterval                 object\n",
      "dtype: object \n",
      "\n",
      "In Store the variable StoreType has numeric values and most of the variable are floats we nee to convert that as integer and datatype also have to be changed for some columns \n"
     ]
    }
   ],
   "source": [
    "# Print data types for each variable in both the data-set\n",
    "print(\"About Data-types\\n\")\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"object: Object format means variables are categorical.\")\n",
    "print(\"int64: It represents the integer variables.\")\n",
    "print(\"float64: It represents the variable which have some decimal values involved. They are also numerical variables.\\n\\n \")\n",
    "print(\"Train Variable Data-types\\n\")\n",
    "print(train.dtypes,'\\n')\n",
    "print(\"Change the Date variable (object) to (datetime64) format because its in datetime format and we can also extract features like day,month,year etc.,\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Test Variable Data-types\\n\")\n",
    "print(test.dtypes,'\\n')\n",
    "print(\"Similarly the data type of Date column to datetime format and Open variable has change to int and StateHoliday values are string type try to keep as int \")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Store Variable Data-types\\n\")\n",
    "print(store.dtypes,'\\n')\n",
    "print(\"In Store the variable StoreType has numeric values and most of the variable are floats we nee to convert that as integer and datatype also have to be changed for some columns \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Datasets\n",
      "--------------------------------------------\n",
      "Train data we have 1017209 rows and 9 Columns \n",
      "Test data we have 41088 rows and 8 Columns \n",
      "Store data we have 1115 rows and 10 Columns \n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Datasets\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Train data we have %s rows and %s Columns \"%(train.shape[0],train.shape[1]))\n",
    "print(\"Test data we have %s rows and %s Columns \"%(test.shape[0],test.shape[1]))\n",
    "print(\"Store data we have %s rows and %s Columns \"%(store.shape[0],store.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in Train data\n",
      "-----------------------------------------\n",
      "Store            0\n",
      "DayOfWeek        0\n",
      "Date             0\n",
      "Sales            0\n",
      "Customers        0\n",
      "Open             0\n",
      "Promo            0\n",
      "StateHoliday     0\n",
      "SchoolHoliday    0\n",
      "dtype: int64\n",
      "Null Values in Test data\n",
      "-----------------------------------------\n",
      "Id                0\n",
      "Store             0\n",
      "DayOfWeek         0\n",
      "Date              0\n",
      "Open             11\n",
      "Promo             0\n",
      "StateHoliday      0\n",
      "SchoolHoliday     0\n",
      "dtype: int64\n",
      "Null Values in Store data\n",
      "-----------------------------------------\n",
      "Store                          0\n",
      "StoreType                      0\n",
      "Assortment                     0\n",
      "CompetitionDistance            3\n",
      "CompetitionOpenSinceMonth    354\n",
      "CompetitionOpenSinceYear     354\n",
      "Promo2                         0\n",
      "Promo2SinceWeek              544\n",
      "Promo2SinceYear              544\n",
      "PromoInterval                544\n",
      "dtype: int64\n",
      "From the above three dataset we see that there is NA Values Present in it we impute those values with knn imputation\n"
     ]
    }
   ],
   "source": [
    "# Check if any Null-Values present in data\n",
    "print(\"Null Values in Train data\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(train.isnull().sum())\n",
    "print(\"Null Values in Test data\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(test.isnull().sum())\n",
    "print(\"Null Values in Store data\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(store.isnull().sum())\n",
    "print(\"From the above three dataset we see that there is NA Values Present in it we impute those values with knn imputation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Columns to Numeric Using Mapping\n",
    "State_unique = train['StateHoliday'].unique()\n",
    "State_mapping = {StateHoliday: idx for idx, StateHoliday in enumerate(State_unique,1)}\n",
    "train['StateHoliday'] = train['StateHoliday'].map(State_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_unique_1 = test['StateHoliday'].unique()\n",
    "State_mapping_1 = {StateHoliday: idx for idx, StateHoliday in enumerate(State_unique_1,1)}\n",
    "test['StateHoliday'] = test['StateHoliday'].map(State_mapping_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoInterval_unique = store['PromoInterval'].unique()\n",
    "PromoInterval_mapping = {PromoInterval: idx for idx, PromoInterval in enumerate(PromoInterval_unique,1)}\n",
    "store['PromoInterval'] = store['PromoInterval'].map(PromoInterval_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But if you see the class '1' represented is still a Nan value so replace with NAN\n",
    "store['PromoInterval'] = store['PromoInterval'].replace(1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_unique = store['StoreType'].unique()\n",
    "store_mapping = {StoreType: idx for idx, StoreType in enumerate(store_unique, 1)}\n",
    "store['StoreType'] = store['StoreType'].map(store_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Assortment_unique = store['Assortment'].unique()\n",
    "Assortment_mapping = {Assortment: idx for idx, Assortment in enumerate(Assortment_unique, 1)}\n",
    "store['Assortment'] = store['Assortment'].map(Assortment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every dataset now only has values after doing mapping(to convert string to numbers)\n"
     ]
    }
   ],
   "source": [
    "PromoInterval_unique = store['PromoInterval'].unique()\n",
    "PromoInterval_mapping = {PromoInterval: idx for idx, PromoInterval in enumerate(PromoInterval_unique, 1)}\n",
    "store['PromoInterval'] = store['PromoInterval'].map(PromoInterval_mapping)\n",
    "print(\"Every dataset now only has values after doing mapping(to convert string to numbers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "store['PromoInterval'] = store['PromoInterval'].replace(1,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First convert the data to array format and apply Knn imputer to it\n",
    "x = store.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer()\n",
    "x = imputer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.DataFrame(x,columns=['Store','StoreType','Assortment','CompetitionDistance','CompetitionOpenSinceMonth',\\\n",
    "                                 'CompetitionOpenSinceYear','Promo2','Promo2SinceWeek','Promo2SinceYear','PromoInterval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Test data also have na value so try to impute it\n",
    "test['Open'].fillna(test['Open'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputing NA Values\n",
      "Null Values in Train data\n",
      "-----------------------------------------\n",
      "Store            0\n",
      "DayOfWeek        0\n",
      "Date             0\n",
      "Sales            0\n",
      "Customers        0\n",
      "Open             0\n",
      "Promo            0\n",
      "StateHoliday     0\n",
      "SchoolHoliday    0\n",
      "dtype: int64\n",
      "Null Values in Test data\n",
      "-----------------------------------------\n",
      "Id               0\n",
      "Store            0\n",
      "DayOfWeek        0\n",
      "Date             0\n",
      "Open             0\n",
      "Promo            0\n",
      "StateHoliday     0\n",
      "SchoolHoliday    0\n",
      "dtype: int64\n",
      "Null Values in Store data\n",
      "-----------------------------------------\n",
      "Store                        0\n",
      "StoreType                    0\n",
      "Assortment                   0\n",
      "CompetitionDistance          0\n",
      "CompetitionOpenSinceMonth    0\n",
      "CompetitionOpenSinceYear     0\n",
      "Promo2                       0\n",
      "Promo2SinceWeek              0\n",
      "Promo2SinceYear              0\n",
      "PromoInterval                0\n",
      "dtype: int64\n",
      "After Imputing No Na-Values in the Dataset\n"
     ]
    }
   ],
   "source": [
    "# Now Check the Na Value in every Dataset\n",
    "print(\"After imputing NA Values\")\n",
    "print(\"Null Values in Train data\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(train.isnull().sum())\n",
    "print(\"Null Values in Test data\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(test.isnull().sum())\n",
    "print(\"Null Values in Store data\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(store.isnull().sum())\n",
    "print(\"After Imputing No Na-Values in the Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Data One-by-One\n",
    "train_data = pd.merge(train,store,how = 'left' , on = 'Store')\n",
    "test_data = pd.merge(test,store,how = 'left' , on= 'Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Datasets Before Merging\n",
      "Train data we have 1017209 rows and 9 Columns \n",
      "Test data we have 41088 rows and 8 Columns \n",
      "--------------------------------------------\n",
      "Shape of the Datasets After Merging\n",
      "Train data we have 1017209 rows and 18 Columns \n",
      "Test data we have 41088 rows and 17 Columns \n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Datasets Before Merging\")\n",
    "print(\"Train data we have %s rows and %s Columns \"%(train.shape[0],train.shape[1]))\n",
    "print(\"Test data we have %s rows and %s Columns \"%(test.shape[0],test.shape[1]))\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Shape of the Datasets After Merging\")\n",
    "print(\"Train data we have %s rows and %s Columns \"%(train_data.shape[0],train_data.shape[1]))\n",
    "print(\"Test data we have %s rows and %s Columns \"%(test_data.shape[0],test_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in Train data\n",
      "-----------------------------------------\n",
      "Store                        0\n",
      "DayOfWeek                    0\n",
      "Date                         0\n",
      "Sales                        0\n",
      "Customers                    0\n",
      "Open                         0\n",
      "Promo                        0\n",
      "StateHoliday                 0\n",
      "SchoolHoliday                0\n",
      "StoreType                    0\n",
      "Assortment                   0\n",
      "CompetitionDistance          0\n",
      "CompetitionOpenSinceMonth    0\n",
      "CompetitionOpenSinceYear     0\n",
      "Promo2                       0\n",
      "Promo2SinceWeek              0\n",
      "Promo2SinceYear              0\n",
      "PromoInterval                0\n",
      "dtype: int64\n",
      "Null Values in Test data\n",
      "-----------------------------------------\n",
      "Id                           0\n",
      "Store                        0\n",
      "DayOfWeek                    0\n",
      "Date                         0\n",
      "Open                         0\n",
      "Promo                        0\n",
      "StateHoliday                 0\n",
      "SchoolHoliday                0\n",
      "StoreType                    0\n",
      "Assortment                   0\n",
      "CompetitionDistance          0\n",
      "CompetitionOpenSinceMonth    0\n",
      "CompetitionOpenSinceYear     0\n",
      "Promo2                       0\n",
      "Promo2SinceWeek              0\n",
      "Promo2SinceYear              0\n",
      "PromoInterval                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Now Check the Na Value after merging \n",
    "print(\"Null Values in Train data\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(train_data.isnull().sum())\n",
    "print(\"Null Values in Test data\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def feature_engg(data):\n",
    "    \n",
    "    data['Year'] = pd.DatetimeIndex(data['Date']).year\n",
    "    data['Month'] = pd.DatetimeIndex(data['Date']).month\n",
    "    data['Day'] = pd.DatetimeIndex(data['Date']).day\n",
    "    data['WeekOfYear'] = pd.DatetimeIndex(data['Date']).weekofyear\n",
    "    data['WeekDay_name'] = pd.DatetimeIndex(data['Date']).weekday_name\n",
    "    mappings2 = {'Sunday':0, 'Monday':1, 'Tuesday':2, 'Wednesday':3, 'Thursday':4, 'Friday':5, 'Saturday':6}\n",
    "    data[\"WeekDay_name\"].replace(mappings2, inplace=True)\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n",
    "        (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    data['CompetitionOpen'] = data.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\n",
    "    data['PrvsHoliday'] = np.where((data[\"StateHoliday\"].shift(1)==1) | (data[\"SchoolHoliday\"].shift(1)==1),1,0)\n",
    "    data['NxtHoliday'] = np.where((data[\"StateHoliday\"].shift(-1)==1) | (data[\"SchoolHoliday\"].shift(-1)==1),1,0)\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n",
    "        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data[\"PromoOpen\"].apply(lambda x: x if x > 0 else 0)\n",
    "    data.loc[data.Promo2SinceYear == 0, 'PromoOpen'] = 0\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Features and Create a New DataFrame For Modelling\n",
    "train_df = feature_engg(train_data)\n",
    "test_df = feature_engg(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape after feature extraction on Train we have 1017209 rows and 27 Columns \n",
      "The Shape after feature extraction on Test we have 41088 rows and 26 Columns \n"
     ]
    }
   ],
   "source": [
    "print(\"The Shape after feature extraction on Train we have %s rows and %s Columns \"%(train_df.shape[0],train_df.shape[1]))\n",
    "print(\"The Shape after feature extraction on Test we have %s rows and %s Columns \"%(test_df.shape[0],test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop([\"Store\",\"CompetitionOpenSinceMonth\",\"CompetitionOpenSinceYear\",\"Customers\",\"Promo2SinceWeek\",\"Promo2SinceYear\",\"Date\",\"Open\"],axis=1)\n",
    "test_df = test_df.drop([\"Store\",\"CompetitionOpenSinceMonth\",\"CompetitionOpenSinceYear\",\"Promo2SinceWeek\",\"Promo2SinceYear\",\"Date\",\"Id\",\"Open\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Dropping Columns the Final Train we have 1017209 rows and 19 Columns \n",
      "After Dropping Columns the Final Test we have 41088 rows and 18 Columns \n"
     ]
    }
   ],
   "source": [
    "print(\"After Dropping Columns the Final Train we have %s rows and %s Columns \"%(train_df.shape[0],train_df.shape[1]))\n",
    "print(\"After Dropping Columns the Final Test we have %s rows and %s Columns \"%(test_df.shape[0],test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['Sales']\n",
    "X = train_df.drop('Sales',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions X_train dataset:  (712046, 18)\n",
      "Number transactions y_train dataset:  (712046,)\n",
      "Number transactions X_test dataset:  (305163, 18)\n",
      "Number transactions y_test dataset:  (305163,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1235)\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = dt.predict(X_train)\n",
    "test_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-Square is 1.0\n",
      "Test R-Square is 0.8765261869367436\n",
      "Mean Squared Error 1831662.7034830565\n",
      "Root Mean Square Error 1353.3893392084394\n",
      "Root Mean Percantage Square Error 36.78844029322852 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "import math\n",
    "mse = mean_squared_error(y_test, test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "rmspe = np.sqrt(rmse)\n",
    "r_train = r2_score(y_train,train_pred)\n",
    "r_test = r2_score(y_test, test_pred)\n",
    "print(\"Train R-Square is %s\"%(r_train))\n",
    "print(\"Test R-Square is %s\"%(r_test))\n",
    "print(\"Mean Squared Error %s\"%(mse))\n",
    "print('Root Mean Square Error %s'%(rmse))\n",
    "print('Root Mean Percantage Square Error %s '%(rmspe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_rf = rf.predict(X_train)\n",
    "test_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-Square is 0.9909676483222494\n",
      "Test R-Square is 0.9360561030879081\n",
      "Mean Squared Error 948570.7793703657\n",
      "Root Mean Square Error 973.9459838052445\n",
      "Root Mean Percantage Square Error 31.20810766139537 \n"
     ]
    }
   ],
   "source": [
    "rf_mse = mean_squared_error(y_test, test_pred_rf)\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "rf_rmspe = np.sqrt(rf_rmse)\n",
    "r_train_rf = r2_score(y_train,train_pred_rf)\n",
    "r_test_rf = r2_score(y_test, test_pred_rf)\n",
    "print(\"Train R-Square is %s\"%(r_train_rf))\n",
    "print(\"Test R-Square is %s\"%(r_test_rf))\n",
    "print(\"Mean Squared Error %s\"%(rf_mse))\n",
    "print('Root Mean Square Error %s'%(rf_rmse))\n",
    "print('Root Mean Percantage Square Error %s '%(rf_rmspe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_lr = lr.predict(X_train)\n",
    "test_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-Square is 0.4582987141425915\n",
      "Test R-Square is 0.4604162438015761\n",
      "Mean Squared Error 8004413.382193165\n",
      "Root Mean Square Error 2829.2072002936025\n",
      "Root Mean Percantage Square Error 53.190292350142265 \n"
     ]
    }
   ],
   "source": [
    "lr_mse = mean_squared_error(y_test, test_pred_lr)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_rmspe = np.sqrt(lr_rmse)\n",
    "r_train_lr = r2_score(y_train,train_pred_lr)\n",
    "r_test_lr = r2_score(y_test, test_pred_lr)\n",
    "print(\"Train R-Square is %s\"%(r_train_lr))\n",
    "print(\"Test R-Square is %s\"%(r_test_lr))\n",
    "print(\"Mean Squared Error %s\"%(lr_mse))\n",
    "print('Root Mean Square Error %s'%(lr_rmse))\n",
    "print('Root Mean Percantage Square Error %s '%(lr_rmspe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:18:45] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_xgb = xgb.predict(X_train)\n",
    "test_pred_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-Square is 0.639711851848096\n",
      "Test R-Square is 0.6401743274538941\n",
      "Mean Squared Error 5337806.031962085\n",
      "Root Mean Square Error 2310.3692414768\n",
      "Root Mean Percantage Square Error 48.066300476287964 \n"
     ]
    }
   ],
   "source": [
    "xgb_mse = mean_squared_error(y_test, test_pred_xgb)\n",
    "xgb_rmse = np.sqrt(xgb_mse)\n",
    "xgb_rmspe = np.sqrt(xgb_rmse)\n",
    "r_train_xgb = r2_score(y_train,train_pred_xgb)\n",
    "r_test_xgb = r2_score(y_test, test_pred_xgb)\n",
    "print(\"Train R-Square is %s\"%(r_train_xgb))\n",
    "print(\"Test R-Square is %s\"%(r_test_xgb))\n",
    "print(\"Mean Squared Error %s\"%(xgb_mse))\n",
    "print('Root Mean Square Error %s'%(xgb_rmse))\n",
    "print('Root Mean Percantage Square Error %s '%(xgb_rmspe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Train Prediction\n",
    "train_pred = pd.DataFrame(train_pred) \n",
    "train_pred_rf = pd.DataFrame(train_pred_rf) \n",
    "train_pred_lr = pd.DataFrame(train_pred_lr) \n",
    "train_pred_xgb = pd.DataFrame(train_pred_xgb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Test Prediction\n",
    "test_pred = pd.DataFrame(test_pred)\n",
    "test_pred_rf = pd.DataFrame(test_pred_rf)\n",
    "test_pred_lr = pd.DataFrame(test_pred_lr)\n",
    "test_pred_xgb = pd.DataFrame(test_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8765261869367472"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train_pred, train_pred_rf,train_pred_lr,train_pred_xgb], axis=1) \n",
    "df_test = pd.concat([test_pred, test_pred_rf,test_pred_lr,test_pred_xgb], axis=1) \n",
    "model = LinearRegression() \n",
    "model.fit(df,y_train) \n",
    "model.score(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
